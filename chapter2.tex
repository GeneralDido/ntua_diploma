\chapter{\tl{CART} }

\section{Εισαγωγή}

Τα μοντέλα \tl{CART (Classification \& Regression Trees)} είναι από τις πιο δημοφιλείς μεθόδους για \tl{tree-based regression} και \tl{classification}. Τα μοντέλα αυτά χρησιμοποιούν αναδρομική διχοτόμηση (\tl{binary recursive partitioning}) ώστε να διαχωρίσουν τα δεδομένα σε υποσύνολα, έτσι ώστε οι καταγραφές εντός των υποσυνόλων να είναι πιο ομοιογενείς μεταξύ τους συγκρινόμενες με το πώς θα ήταν στο ίδιο υποσύνολο \citepri{spssclementine}. 

Σε κάθε βήμα της διαδικασίας επιλέγουμε μία συγκεκριμένη μεταβλητή και ένα σημείο διαχωρισμού και ύστερα διαχωρίζουμε τα δεδομένα μας ή ένα σύνολο αυτών σε δύο μέρη. Αυτό επιτυγχάνεται επιλέγοντας ένα σύνολο προς διαίρεση και εξετάζοντας όλες τις πιθανές μεταβλητές και όλα τα πιθανά σημεία διαχωρισμού αυτών των μεταβλητών. Ύστερα επιλέγουμε τον συνδυασμό μεταβλητής - σημείου διαχωρισμού ο οποίος βελτιστοποιεί κάποιο κριτήριο και με βάση τον συνδυασμό αυτό διαχωρίζουμε το σύνολο σε δύο μέρη και επαναλαμβάνουμε την διαδικασία ανδρομικά. Το σύνηθες κριτήριο για ένα δέντρο παλινδρόμησης είναι το υπολειπόμενο άθροισμα τετραγώνων (\tl{RSS}) ενώ για ένα δέντρο ταξινόμησης είναι ο δείκτης \tl{Gini} (\citealpsec{unc_edu}).

Έστω για παράδειγμα ένα πρόβλημα παλινδρόμησης με συνεχής απόκριση \tl{Y} και εισόδους $\tl{X}_{1}$ και $\tl{X}_{2}$. Αρχικά χωρίζουμε το σύνολο σε δύο υποσύνολα στο σημείο $\tl{X}_{1} = \tl{t}_{1}$. Ύστερα η περιοχή $\tl{X}_{1} \leq \tl{t}_{1}$ χωρίζεται στο $\tl{X}_{2} = \tl{t}_{2}$ και η περιοχή $\tl{X}_{1} > \tl{t}_{1}$ στο σημείο $\tl{X}_{1} = \tl{t}_{3}$. Τέλος, η περιοχή $\tl{X}_{1} > \tl{t}_{3}$ χωρίζεται στο σημείο $\tl{X}_{2} = \tl{t}_{4}$. Η διαδικασία δίνεται σχηματικά στο σχήμα \ref{figure:2}. Το αποτέλεσμα αυτής είναι ο διαχωρισμός του συνόλου σε πέντε περιοχές $\tl{R}_{1}$,$\tl{R}_{2}$,\dots ,$\tl{R}_{5}$ όπως φαίνεται στο σχήμα \ref{figure:3}. Η τιμή της απόκρισης είναι η μέση τιμή κάθε μίας εκ των πέντε περιοχών, $\overline{\tl{y}_{1}},\overline{\tl{y}_{2}},\dots ,\overline{\tl{y}_{5}}$.

Επομένως σε περιοχή $R_{m}$ το μοντέλο παλινδρόμησης προβλέπει την έξοδο \tl{Y} μέσω σταθεράς $c_{m}$ σύμφωνα με την σχέση:
\begin{equation}
	\widehat{f}(x) = \sum_{m=1}^{5}c_{m}I\left \{ (X_{1},X_{2}) \in R_{m} \right \}
\end{equation}

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{fig/rec.png}}%
  \caption{Παράδειγμα \tl{recursive partitioning} με δύο \tl{predictors}}
  \label{figure:2}
\end{figure}

\begin{figure}[!ht] \center\leavevmode
%\epsfxsize=12cm \epsfysize=14cm
\epsfbox{fig/rec2.png} \caption{Περιοχές στο επίπεδο $\tl{X}_{1}$, $\tl{X}_{2}$ μετά από \tl{recursive partition}}\label{figure:3}
\end{figure}

\begin{figure}[!ht] \center\leavevmode
%\epsfxsize=12cm \epsfysize=14cm
\epsfbox{fig/rec3.png} \caption{Δυαδικό δέντρο μετά από \tl{recursive partition}}\label{figure:4}
\end{figure}

Το \tl{I} είναι ένας δείκτης που δείχνει αν η παρατήρηση έγκειται σε κάποια δεδομένη ορθογώνια περιοχή. Επειδή μία παρατήρηση μπορεί να ανήκει κάθε φορά σε μόνο μία από τις πέντε περιφέρειες, τέσσερις από τους πέντε όρους του αθροίσματος θα είναι μηδενικοί. Ως αποτέλεσμα παίρνουμε το $\overline{y}$ της περιοχής που ανήκει το συγκεκριμένο σημείο.
\newpage
Το ίδιο μοντέλο μπορεί να αναπαρασταθεί ως δέντρο, αφού σε κάθε αναδρομική κλήση μπορούμε να διαχωρίσουμε μόνο περιοχές που έχουν διαχωριστεί από πριν. Οι παρατηρήσεις που ικανοποιούν την συνθήκη βρίσκονται στον αριστερό κλάδο και οι υπόλοιπες στον δεξιό. Η κάθε περιοχή $\tl{R}_{i}$ είναι το φύλλο του δέντρου και μπορεί να βρεθεί ακολουθώντας τους κλάδους με τις αντίστοιχες συνθήκες. Τα σημεία διαχωρισμού ονομάζονται \tl{splits}. Το βασικό πλεονέκτημα των δυαδικών δέντρων είναι η ευκολία ερμηνίας τους ακόμη και αν έχουμε ως είσοδο περισσότερες μεταβλητές \citepri{breiman1984classification}. Μία παρουσίαση του δέντρου του προηγούμενου παραδείγματος φαίνεται στην εικόνα \ref{figure:4}.
\section{Κέρδος πληροφορίας}

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{fig/infogain.png}}%
  \caption{Κέρδος πληροφορίας σε διακριτή κατανομή. (\tl{a}) Αρχικό σύνολο δεδομένων $S$. (\tl{b}) Μετά από οριζόντιο διαχωρισμό. (\tl{c}) Μετά από κατακόρυφο διαχωρισμό.  }
  \label{figure:-1}
\end{figure}

Πριν μιλήσουμε για τον αλγόριθμο διαχωρισμού, είναι σημαντικό να εξοικειωθούμε με τις έννοιες της εντροπίας και του κέρδους πληροφορίας (\tl{information gain}), οι οποίες χρησιμοποιούνται συχνά στην στατιστική και στη θεωρία πληροφορίας. Το σχήμα \ref{figure:-1} μας δείχνει ένα σύνολο δεδομένων στον 2\tl{D} χώρο. Διαφορετικά χρώματα υπονοούν διαφορετικές κλάσεις. Στο σχήμα \ref{figure:-1}\tl{a} η κατανομή των κλάσεων είναι ομοιόμορφη διότι έχουμε ακριβώς τον ίδιο αριθμό σημείων σε κάθε κλάση. Αν διαιρέσουμε τα δεδομένα μας οριζόντια όπως στο σχήμα \ref{figure:-1}\tl{b} τότε παράγουμε δύο σύνολα δεδομένων. Κάθε υποσύνολο που δημιουργείται έχει χαμηλότερη εντροπία, αφού έχουμε περισσότερες πληροφορίες σε μικρότερα ιστογράμματα.
\newpage
Το κέρδος πληροφορίας που έχουμε με το να διαιρέσουμε τα δεδομένα είναι \citepri{deng2011bias}:
\begin{equation}
I = H(S)-\sum_{i\in\left \{ 1,2 \right \}}\frac{\left | S^i \right |}{\left | S \right |}H(S^i)
\end{equation}
με την εντροπία \tl{Shannon} να ορίζεται ως (\citealpsec{shannon}):
\begin{equation}
H(S)=-\sum_{c\in C}p(c)\log (p(c))
\end{equation}
Τα $S^i$ υποδηλώνουν υποσύνολα του συνόλου δεδομένων, τα οποία έχουν δημιουργηθεί μετά από \tl{split} και έχουν φτάσει σε κάποιο κόμβο $i$ ακολουθώντας κάποιες διακλαδώσεις του δέντρου που προκύπτει ύστερα από τα \tl{split}.

Στο παράδειγμά μας, ένας οριζόντιο \tl{split} δεν διαχωρίζει τόσο καλά τα δεδομένα μας, και έχει ως αποτέλεσμα ένα κέρδος $I=0.4$. Αν όμως διαχωρίσουμε τα δεδομένα κάθετα, όπως στο σχήμα \ref{figure:-1}\tl{c}, τότε έχουμε πολύ καλύτερα αποτελέσματα με μικρότερη εντροπία και μεγαλύτερο κέρδος πληροφορίας ($I=0.69$). Αυτό συμβαίνει διότι με τον κάθετο διαχωρισμό έχουμε παράξει δύο υποσύνολα που το καθένα έχει μόνο 2 κλάσεις, ενώ στην προηγούμενη περίπτωση αυτό δεν κατέστη δυνατό. Με αυτό το παράδειγμα, μπορούμε να δούμε πώς μπορούμε να χρησιμοποιήσουμε το \tl{information gain} ώστε να παράξουμε την διάσπαση που θα μας δώσει τα καλύτερα αποτελέσματα. Η έννοια του κέρδους πληροφορίας αποτελεί και την βάση της μεθόδου \tl{random forest} την οποία θα αναλύσουμε στο κεφάλαιο 4. 

\section{Αλγόριθμος διαχωρισμού}

Τα βήματα που ακολουθούμε κατά την κατασκευή του δέντρου είναι τα ίδια σε περίπτωση παλινδρόμησης ή ταξινόμησης με διαφοροποιήσεις σε συγκεκριμένα σημεία που αφορούν τον τρόπο επιλογής των μεταβλητών όπως και το κριτήριο διαχωρισμού. Ο αλγόριθμος έχει ως εξής: \\

\textbf{Βήμα 1: Επιλογή μεταβλητής και σημείου διαχωρισμού} \hfill \\

\textbf{\tl{Regression}}: Σε περίπτωση ποσοτικής μεταβλητής $m$ με διακριτές τιμές $\xi_{1},\xi_{2},\dots,\xi_{m}$ εξετάζουμε κάθε μεταβλητή ξεχωριστά. Για κάθε μεταβλητή δημιουργούνται δύο σύνολα τιμών: $\left \{ x \leq \xi_{i} \right \}$ και $\left \{ x > \xi_{i} \right \}$, $i=1,2,\dots,m$. Διαιρέσεις συμβαίνουν μόνο μεταξύ διαδοχικών τιμών των δεδομένων. Επομένως εξετάζονται $m-1$ χωρίσματα. 

\textbf{\tl{Classification}}: Σε περίπτωση ποιοτικής μεταβλητής με $m$ κατηγορίες πρέπει να εξετάσουμε όλους τους δυνατούς τρόπους ανάθεσης των κατηγοριών σε δύο σύνολα. Τα σημεία διαχωρισμού μπορεί να βρίσκονται οπουδήποτε και μία κατηγορία μπορεί να ανατεθεί σε οποιοδήποτε σύνολο. Υπάρχουν $2^{m-1}-1$ τρόποι με τους οποίους μπορεί να συμβεί αυτό. \\

\textbf{Βήμα 2: Υπολογισμός κριτηρίου διαχωρισμού κάθε πιθανής διαίρεσης} \hfill \\

\textbf{\tl{Regression}}: Έστω για συνεχή έξοδο $y$ ότι διαλέγουμε την μεταβλητή $X_{j}$ και το σημείο διαχωρισμού $s$ οπότε παίρνουμε τα εξής δύο σύνολα: \begin{equation}
R_{1}(j,s)=\left \{ x | x_{j} \leq  s \right \} , R_{2}(j,s)=\left \{ x | x_{j} >  s \right \}
\end{equation}
Για συνεχείς μεταβλητές η σύνηθης μέθοδος που χρησιμοποιείται είναι το υπολειπόμενο άθροισμα τετραγώνων (\tl{residual sum of squares - RSS}). Πριν διαιρέσουμε τα δεδομένα, υπολογίζουμε το συνολικό υπολειπόμενο άθροισμα τετραγώνων χρησιμοποιώντας την μέση τιμή όλων των αποκρίσεων \citepri{berk2008statistical}.
\begin{equation}
RSS_{0}=\sum (y_{i}-\overline{y})^2
\end{equation}
Για τις παρατηρήσεις που βρίσκονται στην ίδια ομάδα μετράμε την μέση τιμή της απόκρισης. Τότε το \tl{RSS} θα είναι: \begin{equation}
RSS(split)=RSS_{1}+RSS_{2}=\sum_{R_{1}}(y_{i}-\overline{y}_{1})^2 + \sum_{R_{2}}{(y_{i}-\overline{y}_{2})^2}
\end{equation}
Η μεταβλητή $X_{j}$ και το σημείο διαχωρισμού $s$ που θα λάβουμε θα ικανοποιούν:
\begin{equation}
\max [RSS_{0}-RSS(split)]
\end{equation}

\textbf{\tl{Classification}}: Εδώ έχουμε περισσότερα κριτήρια διαχωρισμού. Έστω $y$ μία κατηγορηματική μεταβλητή με $m$ κατηγορίες και έστω:
\begin{center}
$n_{ik}=$ αριθμός παρατηρήσεων τύπου $k$ στον κόμβο $i$ \\
$p_{ik}=$ ποσοστό παρατηρήσεων τύπου $k$ στον κόμβο $i$ 
\end{center}

Τότε μπορούμε να χρησιμοποιήσουμε κάποιο από τα επόμενα κριτήρια:

\begin{enumerate}
\item \tl{Deviance}: \begin{equation} D_{i}=-2\sum_{k}n_{ik}\log p_{ik} \end{equation}
\item \tl{Cross-entropy}: \begin{equation} D_{i}=-2\sum_{k}p_{ik}\log p_{ik} \end{equation}
\item \tl{Gini index}: \begin{equation} D_{i}=1-\sum_{k}p_{ik}^2 \end{equation}
\item \tl{Misclassification error}: \begin{equation} D_{i}=1-p_{ik(i)} \end{equation}
όπου $k(i)$ είναι η κατηγορία στον κόμβο $i$ με τον μεγαλύτερο αριθμό παρατηρήσεων.
\end{enumerate}

Το σφάλμα για κάποιο διαχωρισμό ενός συνόλου είναι το άθροισμα των σφαλμάτων πάνω σε όλους τους κόμβους: $\sum_{i}D_{i}$. Για κάθε ένα από τα παραπάνω κριτήρια το σφάλμα ελαχιστοποιείται όταν όλες οι παρατηρήσεις για κάποιο κόμβο είναι του ίδιου τύπου. Στόχος του δέντρου είναι να διαχωρίσει τα σύνολα με τέτοιο τρόπο ώστε τα τελικά σύνολα να είναι πιο αμιγή σε σχέση με πριν τον διαχωρισμό \citepri{recpartition}. 

Οι μέθοδοι \tl{cross-entropy, deviance} και \tl{gini index} λόγω του ότι είναι διαφορίσιμες είναι πιο προσιτές για αριθμητική βελτιστοποίηση. Επιπρόσθετα, είναι πιο ευαίσθητες σε μεταβολές πιθανοτήτων των κόμβων σε σχέση με το \tl{missclassification rate}.  \\

\textbf{Βήμα 3: Επιπλέον διαχωρισμός των συνόλων} \hfill \\

Έχοντας διαχωρίσει βέλτιστα το σύνολό μας στο προηγούμενο στάδιο, επόμενο βήμα είναι να διαχωρίσουμε καθένα από τα σύνολα που δημιουργήθηκαν αναδρομικά, λαμβάνοντας υπόψη όλους τους δυνατούς κόμβους προς διαίρεση. Είμαστε υποχρεωμένοι να διαιρέσουμε ανάμεσα στα σύνολα που δημιουργήθηκαν από το προηγούμενο στάδιο ώστε τελικά να παράγουμε το δέντρο μας. \\

\textbf{Βήμα 4: Περάτωση της διαδικασίας} \hfill \\

Μέσω του επόμενου κριτηρίου, σε κάποιο σημείο  σταματάμε τις επιπλέον διαιρέσεις και δίνουμε τέλος στον αλγόριθμο παίρνοντας τα τελικά μας αποτελέσματα.

\section{\tl{Cost Complexity Pruning}}

Ο αλγόριθμος που περιγράψαμε είναι άπληστος και βρίσκει το βέλτιστο σημείο διαχωρισμού για το αμέσως επόμενο βήμα χωρίς όμως να λαμβάνει υπόψη δέντρα επόμενων βημάτων και την επίδοσή τους. Για παράδειγμα, θα μπορούσαμε να είχαμε ένα σημείο διαχωρισμού που να μην δίνει πολύ καλό σφάλμα μετά την διαίρεση, αλλά σε κάποιο επόμενο βήμα να δίνει ένα άλλο σημείο διαχωρισμού με σημαντικά μειωμένο σφάλμα και πολύ καλύτερη επίδοση. Επομένως μία στρατηγική του να κάνουμε διαίρεση μόνο αν η μείωση του συνολικού σφάλματος ξεπερνά κάποιο κατώφλι $\varepsilon$ δεν είναι καλή, αφού μπορεί σε κάποιο επόμενο βήμα να έχουμε ένα πολύ καλό \tl{split} που να επιφέρει σημαντικές βελτιώσεις ακρίβειας.

Η στρατηγική που προτιμάται είναι να μεγαλώσουμε ένα πολύ μεγάλο δέντρο $T_{0}$ σταματώντας την διαδικασία διαίρεσης μόνο όταν ένας ελάχιστος αριθμός κόμβων επιτευχθεί και ύστερα να κλαδέψουμε το δέντρο σύμφωνα με το κριτήριο \tl{cost complexity}.

Έστω υποδέντρο $T \subset T_{0}$ το οποίο παράγεται αν κλαδέψουμε το $T_{0}$. Έστω τα φύλλα του δέντρου $n$, όπου κάθε κόμβος $n$ ανήκει στην περιοχή $R_{m}$. Έστω $\mid T \mid$ ο αριθμός των φύλλων του δέντρου $T$.  

Έστω ότι μεγαλώνουμε ένα πολύ μεγάλο δέντρο με $n$ φύλλα. Σύμφωνα με τον αλγόριθμο που περιγράφτηκε νωρίτερα, σε κάθε στάδιο βρίσκουμε το βέλτιστο δέντρο, οπότε αν κλαδέψουμε το δέντρο μπορούμε να δημιουργήσουμε βέλτιστο δέντρο μικρότερου μεγέθους. Έστω $T$ το τρέχον δέντρο και $\mid T \mid$ το πλήθος των σημείων διαχωρισμού του δέντρου. Το κριτήριο \tl{cost complexity} ορίζεται ως εξής:
\begin{equation}
CC(T)=\sum_{terminal ~ nodes}D_{i}+\lambda \mid T \mid
\label{CCT}
\end{equation}
Το $D_{i}$ είναι το κριτήριο \tl{RSS} για \tl{regression} ή ένα από τα κριτήρια που αναφέρθηκαν για \tl{classification} σε κάποιο κόμβο $i$. Η παράμετρος $\lambda\geq0$ είναι μία παράμετρος συντονισμού (\tl{tuning parameter or penalty term}). Όταν αλλάζουμε αυτή την παράμετρο διαλέγουμε διαφορετικού μεγέθους δέντρα από την ακολουθία μας από τα βέλτιστα δέντρα. Μεγάλες τιμές της παραμέτρου έχουν ως αποτέλεσμα μικρότερα δέντρα, και αντίστροφα όσο μικραίνει το $\lambda$. Για $\lambda=0$ έχουμε το αρχικό μας δέντρο. 

Το $\lambda$ παίζει έναν παρόμοιο ρόλο όπως η μεταβλητή $K$ στο \tl{Akaike information criterion} (\tl{AIC}), το οποίο μετρά την σχετική ποιότητα ενός στατιστικού μοντέλου για κάποιο σύνολο δεδομένων και παρέχει έναν τρόπο επιλογής του τελικού μοντέλου ανάλογα με το \tl{fitting} και την πολυπλοκότητα που θέλουμε να πετύχουμε:

\begin{equation}
AIC=-2\log L+2K
\end{equation}

Για κάθε τιμή της παραμέτρου $\lambda$ μπορούμε να δείξουμε πως υπάρχει μοναδικό ελάχιστο υποδέντρο $T_{\lambda}$ τέτοιο ώστε να ελαχιστοποιεί την \ref{CCT}. Για να βρούμε το $T_{\lambda}$ χρησιμοποιούμε το λεγόμενο \tl{weakest link pruning}: κλαδεύουμε διαδοχικά τον κόμβο που παράγει την μικρότερη αύξηση στο $\sum_{n}D_{i}$ μέχρι να φτάσουμε στην ρίζα. Αυτή η διαδικασία μας δίνει μία αλληλουχία από υποδέντρα τα οποία περιέχουν το $T_{\lambda}$ \citepri{quinlan1987simplifying}.

\section{\tl{Cross Validation}}

Εκτίμηση της παραμέτρου $\lambda$ μπορεί να επιτευχθεί με \tl{k-fold cross validation} (συνήθως \tl{five} ή \tl{tenfold}). Με αυτό τον τρόπο μπορούμε να καθορίσουμε πότε να σταματήσουμε να κλαδεύουμε το δέντρο. 

Για κάθε βέλτιστο δέντρο δεδομένου μεγέθους ή για κάθε διακριτή τιμή \tl{CC(T)} κάνουμε \tl{k-fold cross validation}. Για να το πετύχουμε αυτό, διαιρούμε τυχαία τα δεδομένα μας $D$ σε $k$-\tl{subsets} ώστε να ισχύει:
\begin{center}
$D = D^{(1)} \cup D^{(2)} \cup \dots \cup D^{(k)}$
\end{center}
Κάθε φορά αφήνουμε έξω ένα από τα $k$ υποσύνολα $D_{i}$ και εκτελούμε τον αλγόριθμο για τα $k-1$ υποσύνολα παίρνοντας προβλέψεις $\overline{y}_{i}$ ή $\widehat{p}_{ij}$ ανάλογα με το αν έχουμε \tl{regression} ή \tl{classification tree}. Τα δεδομένα που μένουν έξω χρησιμοποιούνται για να υπολογίσουν το σφάλμα. Αυτή η διαδικασία γίνεται $k$-φορές για κάθε υποδέντρο και το σχετικό μέσο σφάλμα \tl{xerror (relative average cross-validation error)} είναι:
\begin{equation}
xerror = \frac{\frac{1}{k}\sum_{k}RSS(k-fold tree)}{RSS(null tree)}
\label{xerror}
\end{equation}
Η διαδικασία μπορεί να σταματήσει με κάποιον από τους ακόλουθους δύο κανόνες:
\textbf{Κανόνας 1}: Διάλεξε την τιμή \tl{CC(T)} που παράγει το δέντρο το οποίο ελαχιστοποιεί την \ref{xerror}. \\
\textbf{Κανόνας 2}: Έστω $xstd$ η τυπική απόκλιση του δέντρου με το ελάχιστο $xerror$. Τότε διάλεξε την πρώτη (μεγαλύτερη) τιμή \tl{CC(T)} τέτοια ώστε:
\begin{equation}
xerror < \min (xerror)+xstd
\end{equation}

\section{Παρατηρήσεις}

Τα δέντρα δεν είναι χρήσιμα για μικρά σύνολα δεδομένων, διότι εκεί για να επιτύχουμε οποιαδήποτε πρόοδο πρέπει να κάνουμε ισχυρές παραδοχές ενώ τα δέντρα δεν κάνουν ποτέ υποθέσεις σχετικές με τα δεδομένα μας. Από την άλλη πλευρά είναι πολύ χρήσιμα σε μεγάλα σύνολα δεδομένων με πολλές μεταβλητές προς επιλογή αφού μπορούν να βρουν πολύπλοκες δομές οι οποίες δεν μπορούν να ανιχνευθούν με τα συνήθη μοντέλα παλινδρόμησης. Ως αποτέλεσμα, μπορούν να μας δώσουν πολύ καλές προβλέψεις. Είναι σημαντικό να σημειώσουμε πως λόγω του τρόπου δημιουργίας του, το τελικό μας δέντρο δεν είναι αναγκαία το βέλτιστο ακόμη και αν όλα τα υποδέντρα του έχουν δημιουργεί με το βέλτιστο τρόπο και με εφαρμογή μεθόδων \tl{pruning} όπως η μέθοδος \tl{cost complexity} που περιγράφτηκε πριν.

Σε σχέση με τα κλασσικά παραμετρικά μοντέλα, τα δέντρα μπορούν να χρησιμοποιηθούν για να προτείνουν πιθανές αλληλεπιδράσεις μεταξύ μεταβλητών. Για παράδειγμα, όταν δύο διαφορετικές μεταβλητές χρησιμοποιούνται για να διαιρέσουν το δέντρο σε διαφορετικά σημεία στον ίδιο κλάδο, αυτό θα μπορούσε να ερμηνευτεί ως μία αλληλεπίδραση μεταξύ αυτών των δύο μεταβλητών. Επιπρόσθετα, αν μία μεταβλητή εμφανίζεται πολλές φορές κατά μήκος του ίδιου κλάδου, αυτό υποδηλώνει πως μπορεί να υπάρχει μία μη γραμμική σχέση μεταξύ αυτής της μεταβλητής και της εξόδου του συστήματος. Τα δέντρα θα μπορούσαμε ακόμη να τα χρησιμοποιήσουμε ώστε να δούμε αν υπάρχει κάποια πρόσθετη δομή που ίσως να έχει αγνοηθεί από τα κλασσικά παραμετρικά μοντέλα \citepri{fielding1999machine}. 

Τα δέντρα έχουν γνωρίσει αρκετές βελτιώσεις. Πρόσφατη προσοχή έχει δοθεί στις λεγόμενες \tl{ensemble methods}, οι οποίες περιλαμβάνουν μεθόδους όπως \tl{boosting, bagging} και \tl{random forests}. Η ιδέα πίσω από αυτές τις μεθόδους είναι να δημιουργήσουμε δέντρα σε μικρότερα δείγματα δεδομένων, και να πάρουμε τον μέσο όρο των προβλέψεων αυτών έπειτα από διαδοχικές επαναλήψεις \citepri{frayman2002solving}.

\newpage
\thispagestyle{empty}