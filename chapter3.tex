\chapter{\tl{MARS} }

\section{Εισαγωγή}

Το \tl{MARS} (\tl{Multivariate Adaptive Regression Splines}) είναι μία προσαρμοστική τεχνική παλινδρόμησης και είναι κατάλληλη για υψηλών διαστάσεων προβλήματα τα οποία περιέχουν μεγάλο αριθμό εισόδων. Το \tl{MARS} μπορεί να θεωρηθεί ως μία γενίκευση της τεχνικής της γραμμικής προοδευτικής παλινδρόμησης για μη γραμμική μοντελοποίηση ή ως μία τροποποίηση της μεθόδου \tl{CART} ώστε να βελτιώσει τις επιδόσεις της τελευταίας στη ρύθμιση της παλινδρόμησης (\citealpsec{mars_example}).

Το \tl{MARS} δημιουργήθηκε από τον \tl{Jerome Friedman} το 1991. Ο όρος '\tl{MARS}' είναι εμπορικό σήμα της εταιρίας \tl{Salford Systems} ενώ για \tl{open source implementations} φέρει το όνομα '\tl{earth}'. Θεωρείται απλούστερο σε σχέση με άλλα μοντέλα όπως \tl{random forests} ή \tl{neural networks}. Αρχικά θα εισάγουμε την τεχνική \tl{MARS} και ύστερα θα κάνουμε την σύνδεση με άλλα μοντέλα όπως το \tl{CART}. 

\section{\tl{MARS} και \tl{Normal Regression}}

Στο σχήμα \ref{figure:5} βλέπουμε μία σύγκριση συναρτήσεων που έχουν παραχθεί μετά από εφαρμογή ενός απλού μοντέλου \tl{MARS} και ενός μοντέλου γραμμικής παλινδρόμησης πάνω στα ίδια δεδομένα. Αρχικά παρατηρούμε πως στο μοντέλο της παλινδρόμησης έχουμε μία γραμμική συνάρτηση η οποία έχει χρησιμοποιηθεί πάνω στο σύνολο των δεδομένων και αφορά όλο το εύρος τους. Αντίθετα, στο μοντέλο \tl{MARS} έχουμε χωρίσει τα δεδομένα μας σε περιοχές και έχουμε βρει μία γραμμική συνάρτηση για κάθε περιοχή, η οποία περιγράφει καλύτερα τα δεδομένα μας πάνω στην συγκεκριμένη περιοχή σε σχέση με το μοντέλο της παλινδρόμησης. Με αυτό τον τρόπο πετυχαίνουμε καλύτερα και πιο ακριβή αποτελέσματα. 
\newpage
Τα χαρακτηριστικά ενός γραμμικού παραμετρικού μοντέλου (\tl{Global Parametric Model}) όπως για παράδειγμα η γραμμική παλινδρόμηση είναι τα παρακάτω (\citealpsec{mars_train}):
\begin{itemize}
  \item Υψηλή ταχύτητα υπολογισμών αλλά με περιορισμένη ευελιξία
  \item Ακριβής μόνο αν το συγκεκριμένο μοντέλο έχει την σωστή λειτουργική δομή (για παράδειγμα παρουσιάζει γραμμικότητα)
  \item Λειτουργεί καλά σε μικρά σύνολα δεδομένων
  \item Όλα τα δεδομένα επιδρούν συνολικά στην κατασκευή του τελικού μοντέλου
\end{itemize}

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{fig/Untitled2.png}}%
  \caption{\tl{MARS} και \tl{Linear Regression}}
  \label{figure:5}
\end{figure}

Ένα μη παραμετρικό μοντέλο σε αντίθεση με πριν δημιουργεί την τελική συνάρτηση τοπικά και όχι συνολικά. Τα χαρακτηριστικά του είναι τα εξής:
\begin{itemize}
\item Προσδιορίζει μία μικρή περιοχή των δεδομένων
\item Συνοψίζει πώς οι μεταβλητές συμπεριφέρονται στην συγκεκριμένη περιοχή
\item Αναπτύσσει ένα ξεχωριστό μοντέλο σε κάθε περιοχή 
\item Έχει την δυνατότητα να επιβάλλει περιορισμούς συνέχειας
\end{itemize}

\section{Περιγραφή της μεθόδου}

Το \tl{MARS} είναι μία μη παραμετρική διαδικασία παλινδρόμησης η οποία δεν κάνει καμία υπόθεση σχετικά με την υποκείμενη σχέση μεταξύ των εξαρτημένων και των ανεξάρτητων μεταβλητών του συστήματος. Αντίθετα οι συντελεστές και οι συναρτήσεις παράγονται μέσα από τα δεδομένα μας. Υπό μία έννοια, η μέθοδος βασίζεται στην τεχνική ''διαίρει και βασίλευε'' (\tl{Divide and Conquer}) η οποία διαχωρίζει τον χώρο εισόδου σε περιοχές, κάθε μία με την δική της εξίσωση παλινδρόμησης. Το γεγονός αυτό καθιστά το 
\tl{MARS} κατάλληλο για προβλήματα με υψηλότερες διαστάσεις εισόδου (δηλαδή με περισσότερες από 2 μεταβλητές) όπου άλλες κλασσικές τεχνικές δεν θα παρουσίαζαν καλή συμπεριφορά (\citealpsec{mars_ref}).

Το \tl{MARS} χρησιμοποιεί τμηματικά γραμμικές συναρτήσεις της μορφής $(x-t)_+$ και $(t-x)_+$ οι οποίες ονομάζονται ''συναρτήσεις βάσης ''. Το ''+'' σημαίνει ότι λαμβάνουμε υπ' όψη μας μόνο το θετικό μέρος των συναρτήσεων, αλλιώς θεωρούμε την συνάρτηση μηδενική. Έχουμε επομένως \citepri{friedman1995introduction}: \\
\begin{center}
$
(x-t)_+ = \begin{cases}
x-t, & \text{αν $x > t$,}\\
0, & \text{διαφορετικά}\\
\end{cases}
$
~,~
$
(t-x)_+ = \begin{cases}
t-x, & \text{αν $x < t$,}\\
0, & \text{διαφορετικά}\\
\end{cases}
$

\end{center}

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{fig/basis_fun2.png}}%
  \caption{Οι συναρτήσεις βάσης $(x-t)_+$ και $(t-x)_+$ του \tl{MARS}}
  \label{figure:6}
\end{figure}

Στο σχήμα \ref{figure:6} έχουμε ένα παράδειγμα των συναρτήσεων $(x-0.5)_+$ και $(0.5-x)_+$.

Οι συναρτήσεις είναι τμηματικά γραμμικές, με κόμπο (\tl{knot}) στο σημείο $t$ και ονομάζονται \tl{linear splines}. Οι δύο συναρτήσεις  $(x-t)_+$ και $(t-x)_+$ αποτελούν ένα \tl{reflected pair}. Η ιδέα είναι να δημιουργήσουμε \tl{reflected pairs} για κάθε είσοδο $X_{j}$ με \tl{knots} σε κάθε παρατηρούμενη τιμή $x_{ij}$ της συγκεκριμένης εισόδου. Επομένως αν πάρουμε συνολικά τις συναρτήσεις βάσεις για κάθε είσοδο θα έχουμε:
\begin{equation}
C =\left \{ (X_{j}-t)_{+},(t-X_{j})_{+} \right \}_{\begin{matrix}
t\in\left \{ x_{1j},x_{2j},\dots,x_{Nj}  \right \}
\\ 
j=1,2,\dots,p.
\end{matrix}
}
\end{equation} 
Αν κάθε τιμή εισόδου είναι διαφορετική τότε θα έχουμε συνολικά $2Np$ συναρτήσεις βάσης. Παρότι κάθε συνάρτηση βάσης εξαρτάται μόνο από ένα συγκεκριμένο $X_{j}$, για παράδειγμα $h(X)=(X_{j}-t)_{+}$, θεωρείται συνάρτηση ολόκληρου του χώρου εισόδου $\mathbb{R}^{p}$.

Οι συναρτήσεις βάσης του συνόλου $C$ μαζί με το γινόμενο τους συνδυάζονται για να παράγουν προβλέψεις δεδομένων των εισόδων. Η γενική συνάρτηση του μοντέλου \tl{MARS} είναι \citepri{friedman1991multivariate}:
\begin{equation}
f(X)=\beta _{0} + \sum_{m=1}^{M}\beta _{m}h_{m}(X)
\label{eq2}
\end{equation}
όπου κάθε $h_{m}(X)$ είναι μία συνάρτηση στο $C$, ή το γινόμενο δύο ή περισσότερων τέτοιων συναρτήσεων.

Δεδομένων των $h_{m}$, οι συντελεστές $\beta _{m}$ υπολογίζονται με την ελαχιστοποίηση του υπολοιπόμενου αθροίσματος τετραγώνων όπως στην κλασσική γραμμική παλινδρόμηση. Για την κατασκευή των συναρτήσεων $h_{m}(X)$ ξεκινούμε από την σταθερή συνάρτηση $h_{0}(X)=1$ και με όλες τις συναρτήσεις στο $C$ ως υποψήφιες. Σε κάθε στάδιο θεωρούμε ως ένα καινούριο ζευγάρι συναρτήσεων το γινόμενο μιας συνάρτησης $h_{m}$ στο σύνολο $M$ με ένα από τα \tl{reflected pairs} στο $C$. Προσθέτουμε στο μοντέλο $M$ τον όρο της μορφής:
\begin{center}
$\widehat{\beta}_{M+1}h_{l}(X)\cdot (X_{j}-t)_{+}+\widehat{\beta}_{M+2}h_{l}(X)\cdot (t-X_{j})_{+}, h_{l}\in M$
\end{center}
ο οποίος πετυχαίνει την μεγαλύτερη μείωση σφάλματος. Οι όροι $\widehat{\beta}_{M+1}$ και $\widehat{\beta}_{M+2}$ υπολογίζονται από τα ελάχιστα τετράγωνα μαζί με τους υπόλοιπους $M+1$ όρους του μοντέλου. Ύστερα τα καλύτερα γινόμενα μπαίνουν στο μοντέλο $M$ και η διαδικασία συνεχίζεται μέχρι το μοντέλο $M$ να έχει κάποιο μέγιστο αριθμό όρων.

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{fig/mars_ex.png}}%
  \caption{Παράδειγμα συνάρτησης ως γινόμενο συναρτήσεων βάσης}
  \label{figure:7}
\end{figure}

Για παράδειγμα, στο πρώτο στάδιο εισάγουμε στο μοντέλο μία συνάρτηση της μορφής $\beta _{1}(X_{j}-t)_{+}+\beta _{2}(t-X_{j})_{+};t \in \left \{ x_{ij} \right \}$ εφόσον το γινόμενο με την σταθερή συνάρτηση $h_{0}(X)=1$ μας δίνει την ίδια συνάρτηση. Έστω η βέλτιστη επιλογή σύμφωνα με τα προηγούμενα πως είναι η $\beta _{1}(X_{2}-x_{72})_{+}+\beta _{2}(x_{72}-X_{2})_{+}$. 
\newpage 
Αυτό το ζευγάρι συναρτήσεων μπαίνει στο σύνολο $M$ και στο επόμενο βήμα θεωρούμε ένα ζευγάρι της μορφής: 
\begin{center}
$h_{m}(X)\cdot(X_{j}-t)_{+}$ ~ και ~ $h_{m}(X)\cdot(t-X_{j})_{+}$, $t \in \left \{ x_{ij} \right \}$
\end{center}
όπου για το $h_{m}$ έχουμε τις εξής επιλογές:
\begin{center}$
\begin{matrix}
h_{0}(X) & = & 1,\\ 
h_{1}(X)& = & (X_{2}-x_{72})_{+}, \\ 
h_{2}(X) & = & (x_{72}-X_{2})_{+}
\end{matrix}$
\end{center}
Η τρίτη επιλογή δημιουργεί συναρτήσεις όπως η $(X_{1}-x_{51})_{+}\cdot(x_{72}-X_{2})_{+}$ που φαίνεται στο σχήμα \ref{figure:7}. 

\section{\tl{Generalized Cross Validation}}

Στο τέλος της διαδικασίας έχουμε ένα μεγάλο μοντέλο της μορφής (\ref{eq2}). Το μοντέλο αυτό στις περισσότερες περιπτώσεις κάνει \tl{overfit} τα δεδομένα, οπότε ακολουθείται μία αντίστροφη διαδικασία διαγραφής (παρόμοια με την διαδικασία κλαδέματος που είδαμε στα \tl{classification trees}) κατά την οποία σε κάθε βήμα ο όρος ο οποίος δημιουργεί την μικρότερη αύξηση του υπολειπόμενου τετραγωνικού σφάλματος διαγράφεται από το μοντέλο. Ως αποτέλεσμα παράγεται ένα εκτιμώμενο βέλτιστο μοντέλο $\widehat{f}_{\lambda}$ για μέγεθος (πλήθος όρων) $\lambda$. Για την εύρεση της βέλτιστης τιμής $\lambda$ θα μπορούσαμε να χρησιμοποιήσουμε \tl{cross-validation}, αλλά για λόγους υπολογιστικού κόστους το \tl{MARS} χρησιμοποιεί το κριτήριο \tl{generalized cross-validation}.
\newpage
Το \tl{generalized cross-validation} ορίζεται ως:
\begin{equation}
GCV(\lambda) = \frac{\sum_{i=1}^{N}(y_{i}-\widehat{f}_{\lambda}(x_{i}))^2}{(1-M(\lambda)/N)^2}
\end{equation}
Η τιμή $M(\lambda)$ είναι ο αριθμός των παραμέτρων του μοντέλου σε ισχύ: αυτό αντιπροσωπεύει τόσο τον αριθμό των όρων του μοντέλου όπως και τον αριθμό των παραμέτρων που χρησιμοποιούνται για την επιλογή των βέλτιστων θέσεων των \tl{knots}. Κάποιες μαθηματικές προσομοιώσεις υποδεικνύουν πως χρειάζονται περίπου τρεις παράμετροι για να επιλέξουν ένα \tl{knot} σε μία τμηματικά γραμμική παλινδρόμηση. Επομένως αν υπάρχουν $r$ ανεξάρτητες γραμμικές συναρτήσεις βάσης στο μοντέλο, και $K$ \tl{knots} έχουν επιλεγεί κατά την εκτέλεση της διαδικασίας, θα έχουμε: $M(\lambda)=r+cK$ όπου $c=3$. Σύμφωνα με αυτό, επιλέγουμε το μοντέλο που κατά την αντίστροφη διαδικασία ελαχιστοποιεί το $GCV(\lambda)$.

\section{Ο αλγόριθμος \tl{MARS} συνοπτικά}

Όπως αναφέρθηκε και πριν το \tl{MARS} είναι μία διαδικασία δύο σταδίων η οποία εφαρμόζεται διαδοχικά μέχρι να παραχθεί ένα επαρκές μοντέλο. Στο πρώτο στάδιο δημιουργούμε το μοντέλο προσθέτοντας συναρτήσεις βάσης και αυξάνοντας την πολυπλοκότητά του μέχρι να φτάσουμε σε ένα συγκεκριμένο σημείο πολυπλοκότητας. Στο δεύτερο στάδιο, ξεκινούμε ''προς τα πίσω'' και αφαιρούμε τις λιγότερο σημαντικές συναρτήσεις σύμφωνα με το κριτήριο \tl{Generalized Cross Validation}. Ο αλγόριθμος έχει ως εξής:
\begin{enumerate}
\item Ξεκίνα με το απλούστερο μοντέλο που περιέχει μόνο την σταθερή συνάρτηση βάσης
\item Αναζήτησε στον χώρο των συναρτήσεων βάσης και για κάθε μεταβλητή και για όλα τα δυνατά \tl{knots} εισήγαγε στο μοντέλο τις συναρτήσεις που μεγιστοποιούν κάποιο κριτήριο ταιριάσματος (\tl{goodness of fit}) όπως για παράδειγμα ελαχιστοποίηση σφάλματος πρόβλεψης
\item Εφάρμοσε αναδρομικά το βήμα 2 έως ότου το μοντέλο αποκτήσει μία προκαθορισμένη μέγιστη πολυπλοκότητα
\item Κλάδεψε το δέντρο διαγράφοντας τις βασικές συναρτήσεις που συμβάλλουν το λιγότερο (\tl{least squares}) στο \tl{fitting} του μοντέλου ώστε να παραχθεί το τελικό μοντέλο.
\end{enumerate}
\newpage
\section{Παρατηρήσεις}
Το \tl{MARS} έχει γίνει πολύ δημοφιλές για την δημιουργία μοντέλων πρόβλεψης για ''δύσκολα'' προβλήματα εξόρυξης δεδομένων, στα οποία οι \tl{predictors} δεν παρουσιάζουν απλές ή μονότονες σχέσεις με τις εξαρτημένες μεταβλητές. Εναλλακτικά μοντέλα αποτελούν τα \tl{CHAID}, \tl{CART}, όπως και τα νευρωνικά δίκτυα (\tl{Neural Networks}) \citepri{lee2001review}. Λόγω του συγκεκριμένου τρόπου με τον οποίο το \tl{MARS} διαλέγει \tl{predictors} (συναρτήσεις βάσης), γενικά συμπεριφέρεται καλά σε περιπτώσεις όπου μοντέλα όπως το \tl{CART} συμπεριφέρονται καλά, δηλαδή όπου ιεραρχικά δομημένες διαδοχικές διασπάσεις στις μεταβλητές δίνουν ακριβείς προβλέψεις. Στην πραγματικότητα, αντί να θεωρούμε το \tl{MARS} ως μία γενίκευση της πολλαπλής παλινδρόμησης, μπορούμε να αναλογιστούμε το \tl{MARS} ως μία γενίκευση των \tl{regression trees} όπου τα ''σκληρά'' \tl{binary splits} μπορούν να αντικατασταθούν από '' ομαλές '' συναρτήσεις βάσης \citepri{munoz2004comparison}.

Μία σημαντική ιδιότητα των συναρτήσεων βάσης είναι η ικανότητά τους να λειτουργούν σε τοπικό επίπεδο. Όταν οι συναρτήσεις βάσεις πολλαπλασιάζονται μεταξύ τους όπως στο σχήμα \ref{figure:7} τότε το αποτέλεσμα είναι μη μηδενικό μόνο σε ένα μικρό μέρος του επιπέδου στο οποίο και οι δύο συναρτήσεις είναι μη μηδενικές. Ως αποτέλεσμα η επιφάνεια στην οποία γίνεται η παλινδρομηση χτίζεται τοπικά μέσω μη μηδενικών στοιχείων όποτε αυτά χρειάζονται. Η χρήση άλλων συναρτήσεων βάσης όπως των πολυωνυμικών παράγει μη μηδενικό γινόμενο και αλλού στον χώρο, οπότε δεν θα λειτουργούσε καλά.

Ένα δεύτερο σημαντικό πλεονέκτημα των συναρτήσεων βάσης αφορά το υπολογιστικό κόστος. Έστω το γινόμενο μίας συνάρτησης στο $M$ με καθένα από τα $N$ \tl{reflected pairs} για είσοδο $X_{j}$. Αυτό φαίνεται να απαιτεί το ταίριασμα $N$ γραμμικών μοντέλων παλινδρόμησης μονής εισόδου καθένα από τα οποία χρησιμοποιεί $O(N)$ υπολογισμούς με αποτέλεσμα να έχουμε πολυπλοκότητα $O(N^2)$. Σε αυτό το σημείο μπορούμε να εκμεταλλευτούμε την μορφή των συναρτήσεων βάσης. Αρχικά ταιριάζουμε το ζεύγος με το δεξιότερο \tl{knot}. Καθώς το \tl{knot} πηγαίνει διαδοχικά μία θέση αριστερά, οι συναρτήσεις βάσεις θα διαφέρουν κατά μηδέν στο αριστερό τμήμα του επιπέδου και κατά μία σταθερά στο δεξί τμήμα. Επομένως σε κάθε βήμα μπορούμε να ενημερώσουμε το μοντέλο σε $Ο(1)$ οπότε μπορούμε να δοκιμάσουμε κάθε πιθανό \tl{knot} με $Ο(Ν)$ υπολογισμούς.

Το μοντέλο \tl{MARS} κατασκευάζεται ιεραρχικά, αφού οι νέες συναρτήσεις δημιουργούνται από γινόμενα με άλλες συναρτήσεις οι οποίες έχουν δημιουργηθεί από γινόμενα προηγούμενων βημάτων και βρίσκονται ήδη στο μοντέλο. Για παράδειγμα ένα γινόμενο με 4 συναρτήσεις μπορεί να γίνει μόνο αν το γινόμενο των τριών εκ των συναρτήσεων βρίσκεται ήδη στο μοντέλο. Επομένως μία αλληλεπίδραση υψηλής τάξης θα υπάρχει μόνο αν τα μικρότερα μέρη αυτής υπάρχουν ήδη στο μοντέλο. Με αυτό τον τρόπο αποφεύγουμε να αναζητούμε εκθετικές λύσεις στο μοντέλο μας. Ακόμη, έχουμε ένα περιορισμό κατά τον σχηματισμό του μοντέλου. Κάθε είσοδος μπορεί να εμφανιστεί μέχρι μία φορά στο γινόμενο. Αυτό εμποδίζει τον σχηματισμό μεγαλύτερης τάξης δυνάμεων εισόδου, ενώ με τις συναρτήσεις βάσης που είναι τμηματικά γραμμικές μπορούμε να δημιουργήσουμε καλή προσέγγιση με πιο σταθερό τρόπο \citepri{cart_mars}.

Παρά το γεγονός πως το \tl{MARS} αρχικά είχε δημιουργηθεί για επίλυση προβλημάτων \tl{regression}, μπορεί εύκολα να χρησιμοποιηθεί και για προβλήματα \tl{classification} λόγω της δυνατότητας του μοντέλου να χειρίζεται πολλαπλές  μεταβλητές εξόδου. Αρχικά κωδικοποιούμε τις κλάσεις εξόδου σε πολλαπλούς δείκτες (\tl{multiple indicator variables}). Για παράδειγμα, θα μπορούσαμε να κωδικοποιήσουμε ως ''1'' την παρατήρηση η οποία ανήκει στην κλάση $k$ και ως ''0'' την παρατήρηση που δεν ανήκει στην κλάση $k$. Ύστερα εφαρμόζουμε τον αλγόριθμο \tl{MARS} πάνω στο μοντέλο ο οποίος καθορίζει ένα κοινό σύνολο συναρτήσεων βάσης για τις εισόδους μας αλλά με διαφορετικούς συντελεστές για κάθε μεταβλητή και παίρνουμε ως αποτέλεσμα κάποιες προβλέψεις με συνεχείς τιμές ή \tl{scores}. Τέλος, αναθέτουμε στη κλάση που πέτυχε το μεγαλύτερο \tl{score} την πρόβλεψή μας. Σημειώνουμε εδώ πως αυτή η εφαρμογή θα δώσει \tl{classifications} με ευριστικό τρόπο και θα λειτουργήσει καλά στην πράξη, όμως δεν βασίζεται σε κάποιο στατιστικό μοντέλο για εξαγωγή πιθανοτήτων που με βάση αυτές θα εφαρμόζαμε το \tl{classification}.

\newpage
\thispagestyle{empty} 

