\chapter{\tl{Random Forests} }

\section{Εισαγωγή}

Τα \tl{random forests} αποτελούν μία μέθοδο μάθησης που μπορεί να χρησιμοποιηθεί τόσο για προβλήματα ταξινόμησης όσο και για προβλήματα παλινδρόμησης. Λειτουργούν με το να κατασκευάζουν ένα πλήθος από δέντρα απόφασης σε δείγμα του συνόλου των δεδομένων κατά τη φάση εκπαίδευσης του μοντέλου και ύστερα να συνυπολογίζουν όλα τα δέντρα για να καθορίσουν την τελική έξοδο. Αποτελούν μία τροποποίηση της μεθόδου \tl{bagging}, η οποία παίρνει πολλά αμερόληπτα μοντέλα με θόρυβο και βρίσκει την μέση τιμή αυτών, μειώνοντας την διακύμανση της εξόδου. 

Τα δέντρα απόφασης έχουν τις ιδιότητες που γνωρίσαμε στο κεφάλαιο 2 και είναι ιδανικοί υποψήφιοι για μεθόδους \tl{bagging} διότι μπορούν να συλλάβουν πολύπλοκες αλληλεπιδράσεις μεταξύ των δεδομένων, ενώ αν μεγαλώσουν αρκετά βαθιά, έχουν σχετικά χαμηλή μεροληψία. Επιπρόσθετα, λόγω του θορύβου που έχουν, ο μέσος όρος τους αποτελεί έναν καλό δείκτη της πραγματικής εξόδου. Κάθε δέντρο που παράγεται λέμε πως είναι \tl{identically distributed}, δηλαδή είναι ανεξάρτητο από τα άλλα δέντρα και παρέχει την ίδια κατανομή πιθανότητας ως προς την τελική έξοδο. Με αυτό τον τρόπο, ο μέσος όρος όλων των δέντρων παρέχει την ίδια μεροληψία με αυτή που παρέχει ένα δέντρο από μόνο του, οπότε πετυχαίνουμε βελτίωση μέσω μείωσης της διακύμανσης \citepri{clauset2011brief}.

\section{Περιγραφή της μεθόδου}

Η διαδικασία που ακολουθείται είναι αρκετά απλή ως προς την κατανόησή της. Πριν από κάθε \tl{split} διαλέγουμε τυχαία κάποιες μεταβλητές εισόδου ως υποψήφιες για διαχωρισμό. Ύστερα, από αυτές τις μεταβλητές επιλέγουμε το \tl{split point} που θα μας δώσει το μεγαλύτερο κέρδος πληροφορίας (όπως ειπώθηκε στο κεφάλαιο 2.2) και την μεταβλητή στην οποία θα κάνουμε τον διαχωρισμό. Έπειτα κάνουμε το \tl{split} και παράγουμε δύο κόμβους - παιδιά οι οποίοι μας δίνουν την βέλτιστη πληροφορία. Η διαδικασία αυτή επαναλαμβάνεται αναδρομικά για κάθε δέντρο και για πολλά διαφορετικά τυχαία δέντρα. Στο τέλος η έξοδός μας βασίζεται πάνω στο σύνολο των δέντρων και αποτελεί τον μέσο όρο τους.

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{fig/rand1.png}}%
  \caption{Εκπαίδευση ενός δυαδικού δέντρου}
  \label{figure:8}
\end{figure}

Στο σχήμα \ref{figure:8} βλέπουμε ένα παράδειγμα εκπαίδευσης ενός \tl{random classification tree}. Αρχικά διαλέγουμε τυχαία κάποια δεδομένα από το σύνολο εκπαίδευσης, και ύστερα $V$-πλήθος δεδομένων χρησιμοποιούνται ώστε να βελτιστοποιήσουν τις παραμέτρους του δέντρου. Τα \tl{split points} που μας δίνουν το καλύτερο κέρδος πληροφορίας χρησιμοποιούνται για να σπάσουμε τους κόμβους του δέντρου σε νέους κόμβους $S^L$ και $S^R$. Παρατηρούμε πως όσο πηγαίνουμε από τον κόμβο προς τα φύλλα η εντροπία της κατανομής μειώνει, δηλαδή αυξάνεται η εμπιστοσύνη ως προς την κατανομή των μεταβλητών σε κλάσεις. Στο παράδειγμα η μεταβλητή έχει πολύ μεγάλη πιθανότητα να ανήκει στην κλάση με κόκκινο χρώμα, αν ακολουθήσουμε την διαδρομή από την ρίζα στο φύλλο $C$. Στην αρχική μας ρίζα όλες οι μεταβλητές έχουν την ίδια κατανομή πιθανότητας.
\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{fig/rand2.png}}%
  \caption{Έξοδος ως το σύνολο (\tl{ensemble}) εκπαίδευσης}
  \label{figure:9}
\end{figure}

Στο σχήμα \ref{figure:9} έχουμε ένα παράδειγμα δημιουργίας εξόδου για κάποιο από τα δεδομένα μας, έστω $\mathbf{v}$. Το κάθε τυχαίο δέντρο παράγει προβλέψεις κατηγοροποίησης για το δεδομένο, οι οποίες έχουν προέλθει ύστερα από διαδοχικές διαιρέσεις. Οι προβλέψεις είναι διαφορετικές για κάθε δέντρο και τα δέντρα εκπαιδεύονται ξεχωριστά (και πιθανόν παράλληλα). Ύστερα κατά την φάση της δοκιμής του μοντέλου, το σημείο $\mathbf{v}$ ωθείται ταυτόχρονα σε όλα τα δέντρα (ξεκινώντας από την ρίζα) μέχρι να φτάσει στο κατάλληλο φύλλο όπου θα λάβει μία εκτίμηση. Στο παράδειγμα, το δέντρο $t=2$ παράγει την πιο σίγουρη πρόβλεψη ότι το σημείο θα ανήκει στην κλάση με πράσινο χρώμα, ενώ το δέντρο $t=3$ έχει μία πιο ομοιόμορφη κατανομή πιθανοτήτων. Η κάθε πρόβλεψη για το φύλλο θα είναι $p_t(c\left | \mathbf{v} \right |)$. 
\newpage
Η τελική μας πρόβλεψη για το σημείο θα χρησιμοποιεί το σύνολο των τυχαίων δέντρων (\tl{ensemble}) και θα είναι απλά ο μέσος όρος των προβλέψεων για το σημείο αυτό, επομένως \citepri{Criminisi2011TR}:
\begin{equation}
p(c\left | \mathbf{v} \right |)=\frac{1}{T}\sum_{t}^{T}p_t(c\left | \mathbf{v} \right |)
\end{equation}

Ένα ερώτημα που προκύπτει είναι γιατί να χρησιμοποιούμε τον μέσο όρο των τυχαίων δέντρων και όχι κάποιο άλλο κριτήριο για την τελική μας έξοδο. Διαισθητικά, ο μέσος όρος ενός μεγάλου αριθμού από τυχαία δέντρα το καθένα από τα οποία μας δίνει μία πρόβλεψη πάνω σε κάποιο σημείο θα παράγει καλύτερα αποτελέσματα από την πρόβλεψη ενός τυχαίου δέντρου, αφού θα λαμβάνει υπόψη πολλές διαφορετικές περιπτώσεις που καταλήγουν στο ίδιο σημείο. Ο \tl{ensemble} μέσος όρος των τυχαίων δέντρων που προέκυψαν από μικρά υποσύνολα των δεδομένων είναι καλύτερος και από το να κάνουμε \tl{split} ολόκληρο το σύνολο (όπως έγινε στο \tl{CART}) για τον ίδιο λόγο με πριν, αφού τα αποτελέσματα που μπορεί να μας δώσει σταθεροποιούνται γύρω από κάποιο σημείο χωρίς μεγάλη διακύμανση ενώ αν κάνουμε την ίδια διαδικασία σε ολόκληρο το σύνολο είναι πολύ πιθανό το \tl{split} που θα πάρουμε όπως και η έξοδος να μην δίνουν τόσο καλά αποτελέσματα. Πιο τεχνικά, όπως ειπώθηκε και πριν, τα τυχαία δέντρα είναι \tl{identically distributed}. Ο μέσος όρος $B$ \tl{identically distributed} μεταβλητών έχει διακύμανση ίση με:
\begin{equation}
\rho \sigma ^2+\frac{1-\rho }{B}\sigma ^2
\end{equation}
\newpage
Καθώς το $Β$ αυξάνει, ο δεύτερος όρος εξαφανίζεται αλλά ο πρώτος μένει, επομένως όσο αυξάνει το σύνολο των δεδομένων μας τόσο λιγότερο μειώνει η διακύμανση. Επίσης, όσο πιο συσχετισμένα είναι τα δέντρα μεταξύ τους τόσο περιορίζονται τα οφέλη του μέσου όρου. Η ιδέα στα \tl{random forests} είναι να μειώσουμε την συσχέτιση ανάμεσα στα δέντρα, χωρίς να μειώσουμε πολύ την διακύμανση. Αυτό πετυχαίνεται μέσω της τυχαίας επιλογής των μεταβλητών. Ειδικότερα πριν από κάθε \tl{split} διαλέγουμε $m\leq p$ από τις μεταβλητές εισόδου τυχαία ως υποψήφιες προς διαχωρισμό. Τυπικές τιμές για $m$ είναι $\sqrt{p}$ έως και 1.

Όταν $B$ τέτοια δέντρα $\left \{ T(x;\Theta_b) \right \}_1^B$ δημιουργηθούν, τότε  ο μέσος όρος των δέντρων αυτών θα είναι η έξοδός μας. H μείωση του $m$ θα μείωνε την συσχέτιση οποιουδήποτε ζεύγους δέντρων στο σύνολο των τυχαίων μας δέντων, οπότε θα μείωνε και την διακύμανση στον μέσο όρο. Αυτός είναι και ο λόγος που τα μοντέλα \tl{CART} δεν παράγουν τόσο καλά αποτελέσματα όσο τα \tl{random forests}: λαμβάνουν υπόψη ένα πολύ μεγάλο σύνολο δεδομένων και όχι μικρότερα σύνολα, οπότε έχουν και μεγαλύτερη διακύμανση στην έξοδο και τα αποτελέσματα δεν είναι πάντα τόσο καλά. Επομένως στα \tl{random forests}, λόγω της μικρής συσχέτισης των δέντρων μεταξύ τους, μέσω του μέσου όρου μπορούμε να πετύχουμε πολύ καλά αποτελέσματα λόγω μείωσης της διακύμανσης. 

\section{Αλγόριθμος \tl{Random Forests}}

Σύμφωνα με τα παραπάνω, μπορούμε να ορίσουμε τον αγλόριθμο για \tl{random forests} ως εξής \citepri{Elem}: \\

Έστω το σύνολο με $N$ δεδομένα $\left \{ (X_1,Y_1),(X_2,Y_2),\dots,(X_N,Y_N) \right \}$ με $\textbf{\tl{X}}_i$ \tl{predictors} (μεταβλητές εισόδου) με $p$-χαρακτηριστικά και $Y_i$ \tl{responses} (μεταβλητές εξόδου) (κεφάλαιο 1.2). Έστω επίσης $B$ το σύνολο των δέντρων που θα δημιουργηθούν. 
\begin{enumerate}
\item Για $b=1$ έως $B$:
\begin{enumerate}
\item Διάλεξε ένα \tl{bootstrap} δείγμα $\mathbf{Z^*}$ μεγέθους $N$ από τα δεδομένα εκπαίδευσης.
\item Ανέπτυξε ένα τυχαίο δέντρο $T_b$ στα δείγμα  που πήρες, με το να επαναλάβεις αναδρομικά τα παρακάτω βήματα για κάθε τερματικό κόμβο του δέντρου, έως ότου φτάσεις σε κάποιο ελάχιστο μέγεθος κόμβων $n_{\min}$.
\begin{enumerate}
\item Διάλεξε $m$ τυχαία χαρακτηριστικά από τα $p$-χαρακτηριστικά.
\item Επέλεξε την καλύτερη μεταβλητή/σημείο διαχωρισμού από τα $m$.
\item Διαίρεσε τον κόμβο σε δύο κόμβους παιδιά.
\end{enumerate}
\end{enumerate}
\item Η έξοδος είναι το σύνολο (\tl{ensemble}) των δέντρων $\left \{ T_b \right \}_1^B$.
\end{enumerate}
\newpage
Για να κάνουμε μία πρόβλεψη στο νέο σημείο $x$:

\textit{\tl{Regression}}: \begin{equation}
\widehat{f}_{rf}^B(x)=\frac{1}{B}\sum_{b=1}^{B}T(x;\Theta _b)
\end{equation}

\textit{\tl{Classification}}: Έστω $\widehat{C}_b(x)$ η πρόβλεψη για την κλάση του $b$-δέντρου. Τότε: 
\begin{equation}
\widehat{C}_{rf}^B(x)= \textup{\tl{majority vote}} \left \{ \widehat{C}_b(x) \right \}_1^B
\end{equation} 

Με τον όρο \tl{bootstraping} εννοούμε την επιλογή ενός τυχαίου υποσυνόλου από το σύνολο δεδομένων για εκπαίδευση και ύστερα την δοκιμή του μοντέλου που έχει εκπαιδευτεί στο υποσύνολο που δεν επιλέχθηκε ώστε να παραχθούν διάφορα συμπεράσματα σχετικά με την ακρίβεια του μοντέλου (για παράδειγμα προκατάληψη (\tl{bias}), διακύμανση, διαστήματα εμπιστοσύνης, σφάλμα πρόβλεψης) \citepri{efron1994introduction}. 

\section{\tl{Out of Bag Samples}}

Ένα σημαντικό χαρακτηριστικό του αλγορίθμου είναι ότι χρησιμοποιεί δείγματα \tl{out of bag (OOB)}, τα οποία ορίζονται ως εξής: \\

\textit{Για κάθε παρατήρηση $z_i = (x_i,y_i)$ δημιούργησε έναν \tl{random forest predictor} μέσω του μέσου όρου των δέντρων που αντιστοιχούν στα δείγματα στα οποία δεν βρίσκεται ο $z_i$.} \\

Μια εκτίμηση σφάλματος \tl{OOB} είναι σχεδόν ταυτόσημη με αυτή που λαμβάνουμε με ένα \tl{N-fold Cross Validation}. Σε αντίθεση όμως με άλλους μη γραμμικούς εκτιμητές, μπορούμε καθώς τροφοδοτούμε το δέντρο σε μία ακολουθία να εκτελούμε παράλληλα \tl{cross validation}. Μόλις το σφάλμα \tl{OOB} σταθεροποιηθεί, η εκπαίδευση μπορεί να σταματήσει.

\section{\tl{Feature Importances}}

Ένα πολύ σημαντικό χαρακτηριστικό του αλγορίθμου \tl{Random Forest} όπως και του αλγορίθμου \tl{CART} είναι η σημαντικότητα των \tl{predictors}. Σε κάθε διαχωρισμό και σε κάθε δέντρο ξεχωριστά, η επιπλέον βελτίωση στην απόδοση του αλγορίθμου μέσω του συγκεκριμένου \tl{predictor} και σύμφωνα με το κριτήριο διαχωρισμού είναι το μέτρο σημαντικότητας της μεταβλητής, και συσσωρεύεται για όλα τα δέντρα στο δάσος και ξεχωριστά για κάθε μεταβλητή. Στον αλγόριθμο \tl{Random Forest} λόγω του κριτηρίου διαχωρισμού, η πιθανότητα όλες οι μεταβλητές να έχουν ρόλο στο τελικό δέντρο, ακόμα και μικρό, είναι πολύ αυξημένη, ειδικά σε σχέση με άλλες μεθόδους όπως το \tl{gradient boosting}. 

Επιπρόσθετα, στον αλγόριθμο \tl{Random Forest} μπορούμε μέσω των \tl{OOB samples} να δημιουργήσουμε ένα μοντέλο σημαντικότητας που μετρά την προβλεπτική ικανότητα της κάθε μεταβλητής. Όταν το δέντρο \tl{b} μεγαλώνει, μέσω των \tl{OOB samples} μπορούμε να μετρήσουμε την ακρίβεια πρόβλεψης. Έπειτα οι τιμές για την μεταβλητή \tl{j} μετατίθενται τυχαία στα \tl{oob} δείγματα και η ακρίβεια υπολογίζεται ξανά. Η μέση τιμή της μείωσης της ακρίβειας ως αποτέλεσμα των μεταθέσεων σε όλα τα δέντρα είναι ένας δείκτης της σημαντικότητας της μεταβλητής \tl{j} στο \tl{random forest}. 

\section{\tl{Random Forests and Overfitting}} 

Όταν στο δείγμα έχουμε μικρό αριθμό σημαντικών μεταβλητών τότε ο αλγόριθμος \tl{Random Forest} εμφανίζει χειρότερη επίδοση κάθε φορά που αυξάνουμε τον αριθμό των μεταβλητών που δίνουν θόρυβο στην έξοδο. Όταν ο αριθμός των σημαντικών μεταβλητών αυξάνεται, έχουμε ισχυρή επίδοση ακόμα και με πολλές μεταβλητές με θόρυβο. Για παράδειγμα, με 6 σημαντικές και 100 μεταβλητές με θόρυβο, η πιθανότητα μία σημαντική μεταβλητή να επιλεγεί σε οποιοδήποτε \tl{split} είναι 0.46, θεωρώντας $m = \sqrt{6+100} \approx 10$.

Ένα ακόμα σημαντικό γεγονός είναι πως ο αλγόριθμος δεν δημιουργεί \tl{overfitting} στα δεδομένα, ακόμα και σε αύξηση των δέντρων \tl{B}. Όπως και στο \tl{bagging}, το \tl{Random Forest} προσεγγίζει την προσδοκία:
\begin{equation}
\widehat{f}_{rf}(x)=E_\Theta T(x;\Theta)=\lim_{B\to\infty}\widehat{f}(x)_{rf}^B
\end{equation}

με μέσο όρο των $Β$ δέντρων στην κατανομή $\Theta$. Η κατανομή για πολύ υψηλές τιμές μπορεί να κάνει \tl{overfit} στα δεδομένα, δημιουργώντας ένα πολύ πλούσιο μοντέλο το οποίο να δίνει επιπλέον πληροφορία  που δεν χρησιμεύει. Όμως η αύξηση της επίδοσης ενός μοντέλου με ελεγχόμενο βάθος δέντρων στον αλγόριθμο \tl{Random Forest} είναι μηδαμινή \citepri{Elem}. Την επίδοση δέντρων με διαφορετικά βάθη στους αλγορίθμους \tl{CART} και \tl{Random Forest} μελετάμε στο κεφάλαιο 7. Πράγματι εκεί παρατηρούμε πως για μεγάλα βάθη η επίδοση του αλγορίθμου \tl{Random Forest} είναι σταθερή, σε αντίθεση με τον αλγόριθμο \tl{CART} που μπορεί να κάνει \tl{overfit} των δεδομένων.